%
% FH Technikum Wien
% !TEX encoding = UTF-8 Unicode
%
% Erstellung von Master- und Bachelorarbeiten an der FH Technikum Wien mit Hilfe von LaTeX und der Klasse TWBOOK
%
% Um ein eigenes Dokument zu erstellen, müssen Sie folgendes ergänzen:
% 1) Mit \documentclass[..] einstellen: Master- oder Bachelorarbeit, Studiengang und Sprache
% 2) Mit \newcommand{\FHTWCitationType}.. Zitierstandard festlegen (wird in der Regel vom Studiengang vorgegeben - bitte erfragen)
% 3) Deckblatt, Kurzfassung, etc. ausfüllen
% 4) und die Arbeit schreiben (die verwendeten Literaturquellen in Literatur.bib eintragen)
%
% Getestet mit TeXstudio mit Zeichenkodierung ISO-8859-1 (=ansinew/latin1) und MikTex unter Windows
% Zu beachten ist, dass die Kodierung der Datei mit der Kodierung des paketes inputenc zusammen passt!
% Die Kodierung der Datei twbook.cls MUSS ANSI betragen!
% Bei der Verwendung von UTF8 muss dnicht nur die Kodierung des Dokuments auf UTF8 gestellt sein, sondern auch die des BibTex-Files!
%
% Bugreports und Feedback bitte per E-Mail an latex@technikum-wien.at
%
% Versionen
% *) V0.7: 9.1.2015, RO: Modeline angepasst und verschoben
% *) V0.6: 10.10.2014, RO: Weitere Anpassung an die UK
% *) V0.5: 8.8.2014, WK: Literaturquellen überarbeitet und angepasst
% *) V0.4: 4.8.2014, WK: Initalversion in SVN eingespielt
%
\documentclass[Bachelor,BIF,english]{twbook}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

%
% Bitte in der folgenden Zeile den Zitierstandard festlegen
\newcommand{\FHTWCitationType}{IEEE} % IEEE oder HARVARD möglich - wenn Sie zwischen IEEE und HARVARD wechseln, bitte die temorären Dateien (aux, bbl, ...) löschen
%
\ifthenelse{\equal{\FHTWCitationType}{HARVARD}}{\usepackage{harvard}}{\usepackage{bibgerm}}

% Definition Code-Listings Formatierung:
\usepackage[final]{listings}
\lstset{captionpos=b, numberbychapter=false,caption=\lstname,frame=single, numbers=left, stepnumber=1, numbersep=2pt, xleftmargin=15pt, framexleftmargin=15pt, numberstyle=\tiny, tabsize=3, columns=fixed, basicstyle={\fontfamily{pcr}\selectfont\footnotesize}, keywordstyle=\bfseries, commentstyle={\color[gray]{0.33}\itshape}, stringstyle=\color[gray]{0.25}, breaklines, breakatwhitespace, breakautoindent}
\lstloadlanguages{[ANSI]C, C++, [gnu]make, gnuplot, Matlab}

%Formatieren des Quellcodeverzeichnisses
\makeatletter
% Setzen der Bezeichnungen für das Quellcodeverzeichnis/Abkürzungsverzeichnis in Abhängigkeit von der eingestellten Sprache
\providecommand\listacroname{}
\@ifclasswith{twbook}{english}
{%
    \renewcommand\lstlistingname{Code}
    \renewcommand\lstlistlistingname{List of Code}
    \renewcommand\listacroname{List of Abbreviations}
}{%
    \renewcommand\lstlistingname{Quellcode}
    \renewcommand\lstlistlistingname{Quellcodeverzeichnis}
    \renewcommand\listacroname{Abkürzungsverzeichnis}
}
% Wenn die Option listof=entryprefix gewählt wurde, Definition des Entyprefixes für das Quellcodeverzeichnis. Definition des Macros listoflolentryname analog zu listoflofentryname und listoflotentryname der KOMA-Klasse
\@ifclasswith{scrbook}{listof=entryprefix}
{%
    \newcommand\listoflolentryname\lstlistingname
}{%
}
\makeatother
\newcommand{\listofcode}{\phantomsection\lstlistoflistings}

% Die nachfolgenden Pakete stellen sonst nicht benötigte Features zur Verfügung
\usepackage{blindtext}
\usepackage{parskip}
\usepackage{epstopdf}
\usepackage{url}
\usepackage{breakurl}

%
% Einträge für Deckblatt, Kurzfassung, etc.
%
\title{Rebuilding a SharePoint 2013 Application with Angular 7 and Reactive Programming}
\author{Dominik Hack}
\studentnumber{1610257044}
\supervisor{DI Thomas Rongitsch}
\place{Vienna}
\kurzfassung{text}
\schlagworte{schlagwort1}
\outline{text}
\keywords{keyword1}

\begin{document}

%Festlegungen für den HARVARD-Zitierstandard
\ifthenelse{\equal{\FHTWCitationType}{HARVARD}}{
\bibliographystyle{Harvard_FHTW_MR}%Zitierstandard FH Technikum Wien, Studiengang Mechatronik/Robotik, Version 1.2e
\citationstyle{dcu}%Correct citation-style (Harvardand, ";" between citations, "," between author and year)
\citationmode{abbr}%use "et al." with first citation
\iflanguage{ngerman}{
    %Deutsch Neue Rechtschreibung
    \newcommand{\citepic}[1]{(Quelle: \protect\cite{#1})}%Zitat: Bild
    \newcommand{\citefig}[2]{(Quelle: \protect\cite{#1}, S. #2)}%Zitat: Bild aus Dokument
    \newcommand{\citefigm}[2]{(Quelle: modifiziert "ubernommen aus \protect\cite{#1}, S. #2)}%Zitat: modifiziertes Bild aus Dokument
    \newcommand{\citep}{\citeasnoun}%In-Line Zitiat entweder mit \citep{} oder \citeasnoun{}
    \newcommand{\acessedthrough}{Verf{\"u}gbar unter:}%Für URL-Angabe
    \newcommand{\acessedthroughp}{Verf{\"u}gbar bei:}%Für URL-Angabe (Geschützte Datenbank, Zugriff durch FH)
    \newcommand{\acessedat}{Zugang am}%Für URL-Datum-Angabe
    \newcommand{\singlepage}{S.}%Für Seitenangabe (einzelne Seite)
    \newcommand{\multiplepages}{S.}%Für Seitenangabe (mehrere Seiten)
    \newcommand{\chapternr}{K.}%Für Kapitelangabe
    \renewcommand{\harvardand}{\&}%Harvardand in Zitaten
    \newcommand{\abstractonly}{ausschließlich Abstract}
    \newcommand{\edition}{. Auflage}%Angabe der Auflage
}{
\iflanguage{german}{
    %Deutsch
    \newcommand{\citepic}[1]{(Quelle: \protect\cite{#1})}%Zitat: Bild
    \newcommand{\citefig}[2]{(Quelle: \protect\cite{#1}, S. #2)}%Zitat: Bild aus Dokument
    \newcommand{\citefigm}[2]{(Quelle: modifiziert "ubernommen aus \protect\cite{#1}, S. #2)}%Zitat: modifiziertes Bild aus Dokument
    \newcommand{\citep}{\citeasnoun}%In-Line Zitiat entweder mit \citep{} oder \citeasnoun{}
    \newcommand{\acessedthrough}{Verf{\"u}gbar unter:}%Für URL-Angabe
    \newcommand{\acessedthroughp}{Verf{\"u}gbar bei:}%Für URL-Angabe (Geschützte Datenbank, Zugriff durch FH)
    \newcommand{\acessedat}{Zugang am}%Für URL-Datum-Angabe
    \newcommand{\singlepage}{S.}%Für Seitenangabe (einzelne Seite)
    \newcommand{\multiplepages}{S.}%Für Seitenangabe (mehrere Seiten)
    \newcommand{\chapternr}{K.}%Für Kapitelangabe
    \renewcommand{\harvardand}{\&}%Harvardand in Zitaten
    \newcommand{\abstractonly}{ausschließlich Abstract}
    \newcommand{\edition}{. Auflage}%Angabe der Auflage
}{
    %Englisch
    \newcommand{\citepic}[1]{(Source: \protect\cite{#1})}%Zitat: Bild
    \newcommand{\citefig}[2]{(Source: \protect\cite{#1}, p. #2)}%Zitat: Bild aus Dokument
    \newcommand{\citefigm}[2]{(Source: taken with modification from \protect\cite{#1}, p. #2)}%Zitat: modifiziertes Bild aus Dokument
    \newcommand{\citep}{\citeasnoun}%In-Line Zitiat entweder mit \citep{} oder \citeasnoun{}
    \newcommand{\acessedthrough}{Available at:}%Für URL-Angabe
    \newcommand{\acessedthroughp}{Available through:}%Für URL-Angabe (Geschützte Datenbank, Zugriff durch FH)
    \newcommand{\acessedat}{Accessed}%Für URL-Datum-Angabe
    \newcommand{\singlepage}{p.}%Für Seitenangabe (einzelne Seite)
    \newcommand{\multiplepages}{pp.}%Für Seitenangabe (mehrere Seiten)
    \newcommand{\chapternr}{Ch.}%Für Kapitelangabe
    \renewcommand{\harvardand}{\&}%Harvardand in Zitaten
    \newcommand{\abstractonly}{Abstract only}
    \newcommand{\edition}{~edition}%Edition -> note, that you have to write "edition = {2nd},"!
}}}

\maketitle
\chapter{Introduction}
describe starting point, why will this project be realised, what should be achieved?, motivation, SharePoint \cite{SharePoint}, SharePoint Requirements (IE), Project Requirements (features), old solution, rebuilding with angular because of new features and same cost of developing a new application and not building on sharepoint feature (also extensibility, maintenance and encapsulation), better for company to have knowledge in Angular than in Sharepoint 2013 (because more projects also use angular), possible research questions: extensibility (1 of 4 team members have experience in developing sharepoint apps, 3 of 4 team members have experience in developing angular apps), maintenance?, observer patter in reactive programming?, overhead of redux?, war angular wirklich schneller zu entwickeln als sharepoint? (Vorteile: Angular wird häufig auch bei anderen Projekten verwendet, no publishing of whole sharepoint project through hosting angular app locally which saves time also angular supports hot reloading)
\clearpage


\chapter{Tools \& Frameworks} 
what will be presented in this chapter (reactive programming; tools to use this paradigm with: ReactiveX, Redux and NgRx; Angular; why typescript; what components are; introduction of important tools used by angular: npm, webpack and babel),
For the implementation of this project a technology had to be decided on that meets the requirements of being integrated into and communicate with SharePoint 2013.

\section{Reactive Programming}
If an interaction with a reactive application takes place, a event occurs which the software will respond on by reacting on it in a certain way. Therefore a reactive software system could be any application with a graphical user interface (GUI) reacting on user input, a network monitoring service reacting on certain changes of the network or even a simple calculator reacting on button presses. reactive applications can become complex because of a mixed combination of data and control flow \cite[p.~1]{PositivEffectOfRP}, 
two main approaches to implement a reactive application (also explain figure):
\begin{figure}[!htbp]
\centering
\includegraphics[width=0.75\linewidth]{PICs/pseudo_Code_reactive_2_ways.eps}
\caption{JavaScript-like pseudo code of the observer pattern approach (left) and the signals and vars approach (right) \cite[p.~797]{DebuggingRP} \cite[p.~3]{PositivEffectOfRP}}\label{Fig1}
\end{figure}
traditional approach observer pattern \cite[p.~360-372]{ObserverDP}, decouples event consumers from event producers \cite[p.~953]{RPWalkthrough}, (in object oriented programming using observer pattern, decouples observers or subscribers from observables or subjects, observers subscribe to observables, observables do not know their subscribers, when observables change their state they notify their observers depending on the implementation this can be done through emitting events. When a subscriber learns of a change in the state of the subject, he adjusts his state to that of the subject. \cite[p.~363]{ObserverDP}),
vs the other way, what is reactive programming: programming paradigm supporting language-level abstractions \cite[p.~953]{RPWalkthrough}, The four principles all implementations of reactive programming pursue are:
\begin{itemize}
\item Declarativeness: declaring how the states of the components depend on each other, not how to retrieve the state when changes occur \cite[p.~2]{PositivEffectOfRP}.
\item Abstraction over change propagation: No need to manually update dependencies, propagation of change is handled by language runtime \cite[p.~2]{PositivEffectOfRP}.
\item Composability: Composing reactive computations through abstractions \cite[p.~2]{PositivEffectOfRP}.
\item Favoring data flow over control flow: Calculations of the software system are triggered by new data or events and are not depending on the execution of the control flow \cite[p.~2]{PositivEffectOfRP}.
\end{itemize}
einiges an Forschung wurde schon betrieben und nach der Einführung von funcitonal reactive programming in Haskell wurde dieses konzept auch in Scheme (FrTime \cite{FrTime}), Scala (Scala.react \cite{DeprecatingOP}) und JavaScript (Flapjax \cite{Flapjax}) implementiert, sogar wurde in Microsofts Reactive Extensions (RX or ReactiveX) konzepte von reactive programming implementiert \cite[p.~954]{RPWalkthrough} \cite[p.~796]{DebuggingRP} \cite[p.~2]{PositivEffectOfRP}
(signals or behaviours: time changing values, treated as constraints of the language runtime, when an inconcistency of a signal or behaviour is detected a recalculation is triggered \cite[p.~797]{DebuggingRP} \cite[p.~3]{PositivEffectOfRP}, can either depend on other signals or vars, vars are reactive values like signals but without dependencies \cite[p.~2]{PositivEffectOfRP}), less error prone, easier to understand, still immature field -> lack of developer tools like debuggers \cite[p.~796]{DebuggingRP}

\subsection{ReactiveX}
general introduction (what is it \cite{ReactiveExtensions} \cite[p.~291]{RxAngular5Prj} (von Microsoft aber open source), why should it be used) \cite{ReactiveX}, bringing functional-reactive-programming-like reactivity (FRP-like reactivity \cite{PositivEffectOfRP}
marble-diagrams
\begin{figure}[!htbp]
\centering
\includegraphics[width=0.5\linewidth]{PICs/marble_diagram.eps}
\caption{An example of a marble diagram, which is mostly used for explaining transformations of observables \cite{RxObservables}.}\label{Fig2}
\end{figure}
observables, observables background with reactor pattern \cite{RxObservables} \cite[p.~292]{RxAngular5Prj} \cite[p.~310]{RxAngular5Prj}

\subsection{Redux}
general introduction (what is it, why should it be used), \cite{Redux}, motivation why redux \cite{ReduxIntroMoti}, core concept \cite{ReduxIntroCC}, three principles \cite{ReduxIntro3P}, figure for explanation

\subsection{NgRx}
general introduction (what is it, why should it be used), \cite{Ngrx}, ng/store, effects


\section{Angular}
what is angular? what is it for? \cite[p.~xxiv-xxix]{RxAngular5Prj}, dependency injection \cite[p.~]{YakovFainAngular} \cite[p.~211]{RxAngular5Prj}, data binding, \cite[p.~]{YakovFainAngular}, bundeling code in modules \cite[p.~116]{RxAngular5Prj}

\subsection{TypeScript}
general introduction \cite[p.~19]{RxAngular5Prj} \cite{TypeScript} \cite[p.~41]{RxAngular5Prj} (what is it, why should it be used), 

\subsection{Components}
general introduction \cite[p.~22]{RxAngular5Prj} \cite[p.~89]{RxAngular5Prj} \cite[p.~159]{RxAngular5Prj} (what is it, why should it be used, structured inside of a module), 

\subsection{npm}
general introduction \cite[p.~71]{RxAngular5Prj} (what is it, why should it be used), 
tool to install everything js based

\subsection{Webpack}
general introduction \cite[p.~113]{RxAngular5Prj} (what is it, why should it be used), 
bundling assets (vendor, main, styles, etc.)

\subsection{Babel}
general introduction (what is it, why should it be used), 
polyfills \cite[p.~18]{RxAngular5Prj}

\clearpage 

\chapter{Implementation}
The following sections deal with describing the processes utilized to ensure the quality of the final product both during the development phase and after completion. First, the general procedures related to project management and continuous integration are illustrated. Subsequently, the software architecture including the project structure, the interfaces and additionally added technologies as well as methodologies will be presented. Finally, the development with reactive programming is discussed.

\section{Project Management}
To successfully complete this project with as few errors as possible and to develop a product that adds business value to the customer and satisfies the customers' needs, the agile project management framework "Scrum" \cite{Scrum} was employed. The scrum team consisted of 3 developers, one of whom was one half of his time involved in scrum master activities, and one product owner. The product owner took care of analyzing the current problems together with the customer in order to develop a possible solution. Based on these requirements, a first version of the backlog was created to give the development team an overview of the product to be developed. Subsequently, the developer team estimated the story points of each user story using planning poker. With this information the product owner calculated the offer for the customer. After the contract was signed the scrum team met for a project kick-off meeting to discuss the highest priority user stories and refine the backlog for the first sprint.
\\[\baselineskip]
The length of a sprint for this project was one week, since at first the user stories were small enough so that the commitment to the sprint goal could be kept. Later, the length of a sprint was increased to two weeks, because on the one hand some user stories were too large to finish in a week and on the other hand a developer had to be assigned to other projects for some time. Sprints were planned using the user-story- and task-board-backlog tools provided by the company's TFS (Team Foundation Server) \cite{TFS}. After each sprint a short retro was held on the day of the planning where the finished user stories and the problems encountered while implementing them were deliberated. In addition when it occurred that a commitment could not be kept, the reasons were sought to make adjustments to user stories that could be affected. 
\\[\baselineskip]
Every day a daily stand-up was held where each developer announced the current status of their to them assigned user stories. If there was any difficulty implementing a user story, after the meeting, the task in question was discussed with at least one other developer. If no solution was found, the product owner was contacted to discuss the problem. This led in most cases to an adaptation of the acceptance criteria of the user story, only rarely the customer had to be contacted to discuss the respective requirement. 
\\[\baselineskip]
If all tasks of a user story were implemented and thus also fulfilled all acceptance criteria, the user story could be closed and a pull request could be created. The pull request was accepted when another developer agreed to the changes and the build process, which is described in more detail in the next chapter, succeeded. So that a user story was now considered as resolved, the product owner together with the developer tested the respective features on the development server. If changes still had to be made, the user story was set to active as long as there was still enough time in the running sprint otherwise it was put back in the backlog to be scheduled for the next sprint. After a certain number of user stories were resolved, a milestone was reached, meaning that a demo meeting with the customer was arranged in order to present the current status of the product. As a result, the current application was deployed on the user acceptance test server and then an in-house demo meeting with the scrum team was held to prepare and practice for the upcoming demo meeting with the customer. The feedback given by the customer during the demo meeting was collected by the product owner, who then adjusted the remaining user stories accordingly. If necessary, new user stories were created and added to the backlog. When all the user stories were resolved, a final demo meeting was held with the client, and after the final adjustments were integrated, the product was deployed to the client's production systems.

\section{Continuous Integration}
In order to detect as many bugs as possible before merging the changes with the master branch of the git \cite{Git} repository and maximizing the quality of the source code as well as the project structure, the development team defined a CI process with specific rules for the project. These rules applied to handling the local cloned git repository and when changes to the source code were accepted for merging with the shared repository. 
\\[\baselineskip]
Before starting the implementation of a user story, a feature branch had to be created, because committing on the master branch was not possible due to constraints and after merging the changes into the master branch, the respective branch had to be deleted. If the acceptance criteria of the user story were met, all changes to the source code had to be pushed to the shared repository and a pull request had to be created using the TFS user interface. The pull request was accepted if at least one other developer agreed, no comments on the source code were still marked as active, and the, on creation of the pull request automatically triggered, build process had been successfully completed. The review of the source code of the pull request was carried out by a code review with at least one other developer. If changes had to be made to the code after the code review, the pull request remained open. Only when all conditions had been fulfilled the pull request was completed.
\\[\baselineskip]
The build pipeline was designed utilizing the build definition user interface of the TFS \cite{TFSBuildPipeline} . The requirements for a build server for this procedure are that it has npm installed and has enough space to download and install the node modules. The build process consists of six tasks which are the following:
\begin{itemize}
\item "Get sources": As the name implies, this build step is responsible for getting the latest source code. When configuring the task, the name of the project, the name of the code repository as well as the name of the branch that should be fetched, after the process started, had to be defined.
\item "npm install": This step executes the command "npm install" and thus installs all required dependencies of the application which are defined in the "package-lock.json" file \cite{NpmInstall}. In the configuration of the step, the folder in which the "package.json" is located was specified, as this allows the task to keep in mind the additional configurations for the particular command to be executed \cite{NpmTaskInstall}.
\item "npm build": The use of this step is it to start a production build process. A custom command must be specified for this step because the build definition tool of the companies TFS did not support the command "npm run build" out of the box and furthermore additional arguments for the command had to be specified. The command that is executed is "npm run prod-build" which is an abbreviation for "ng build --prod --output-path='dist/blog-app'" defined in the "package.json" file. The argument "--prod" sets the production-flag to true and thereby executes a build which does not only employ bundeling and limited tree-shaking but also limited dead code elimination. The argument "--output-path" specifies the relative path to the folder where the compiled project will be moved \cite{AngularCliBuild}. As with the previous step, the folder in which the "package.json" file is located was specified \cite{NpmTaskCustom}.
\item "npm test": In this step, the test process is initiated. For the same reason as with the "npm build" step a custom command has to be configured. The command specified here is "npm run test-headless" which in turn is a short for "ng test --watch=false --browsers=PhantomJS --reporters=progress,junit". The argument "--watch=false" ensures that the test process is terminated after a run. The other two arguments define which reporter or browser should be operated by karma \cite{KarmaConfig}. As a reporter Progress and JUnit \cite{JUnit} \cite{KarmaJUnitReporter} were chosen, because firstly the test results generated by the Progress reporter can be found in the console output of the build job in the TFS and secondly a more pleasant view of the test results is generated by the TFS in the next step by interpreting the XML output of the test results created by the JUnit reporter. The reason why PhantomJS \cite{PhantomJS} was chosen as the browser in which the tests are running will be explained in the next paragraphs. As with the other two steps before, the folder in which the "package.json" file is was specified again \cite{NpmTaskCustom}.
\item "Publish Test Results": The test results created by the previous task are captured by this task and passed to the TFS in the correct format. The configuration of this build step consists of the respective format of the test results, the name of the file in which the test results are to be found and in which folder this file should be searched for. The name of the output file of the JUnit reporter is, unless the default configuration inside of the karma configuration file is changed, in the pattern "TESTS-Name-Of-Browser{\_}Version-Of-Browser{\_}(Name-Of-Operating-System{\_}Version-Of-Operating-System).xml", therefore "TESTS-*.xml", where "*" serves as a wildcard, was set \cite{KarmaJUnitReporterConfig}. The default value "\$(System.DefaultWorkingDirectory)" for the search folder was retained because no changes were made to the configuration of the output folder of the JUnit Reporter \cite{TFSPublishBuildArtifact}.
\item "Publish Artifact: dist": In the last step, the files created by the build task are issued as artifacts. For this, the path to the folder containing the compiled files and the name of the artifact had to be specified. The value for the path was the output folder specified in the "--output-path" argument of the "npm build" command. For the name of the artifact "drop" was selected because it was a rule of the development team for this project to call each build artifact "drop". An artifact created by this task could be downloaded to manually deploy on a SharePoint 2013 development server or User Acceptance Test Server for further testing by a developer, the product owner or the customer \cite{TFSPublishBuildArtifact}.
\end{itemize}
For this project, the first build process for an Angular project of the development team has been created, which in addition to building the application also runs the tests and then publishes the results. This was mostly because the build servers had problems launching a browser like Google Chrome \cite{Chrome} or Mozilla Firefox \cite{Firefox} however, a browser is needed to run the tests with Karma as the test runner. The solution to this problem was to use a headless browser \cite[p.~2]{HeadlessBrowser} for executing the tests. In contrast to browsers like Google Chrome or Mozilla Firefox, headless browsers are always executed in the background and do not have a graphical user interface. The communication with a headless browser thus only takes place via the console, which allows a build server to run it. The selected headless browser for this project was PhantomJS \cite{PhantomJS}, as two developers of the team had already gained experience with PhantomJS.
\\[\baselineskip]
\begin{figure}[!htbp]
\centering
\includegraphics[width=0.5\linewidth]{PICs/karma_phantomjs.eps}
\caption{The "karma.conf.json" file with the configuration to utilize PhantomJS added to it}\label{Fig3}
\end{figure}

For PhantomJS to be operated as a browser for the test task, the three npm packages: phantomjs-prebuilt, karma-phantomjs-launcher, and karma-junit-reporter had to be included in the project and added to the karma configuration file as shown in the figure \ref{Fig3}. These packages install PhantomJs and two karma plugins, which allows Karma to run the tests inside of PhantomJS and create reports of the results which the TFS user interface can interpret and showcase. Additionally, a process variable with the value "C:/NPM/Modules/PhantomJS.cmd" had to be added to the build process, so that the respective agent finds the executable file of PhantomJS.


\section{Integrating Angular in SharePoint 2013}
In order to use Angular as the framework for developing the application, it was first necessary to find a way to host the application in SharePoint 2013. This problem was solved by injecting the compiled project files each inside of one HTML script tag into the script editor web part of the landing page. For the value of the  HTML script tag's source attribute the path to the compiled files and for the value of the type attribute "text/javascript" was defined. As a result, when the page was loaded, the scripts were executed and thereby started the application. An advantage of this solution was that every developer could work locally on the blog application because the path to the compiled project files could also point to the output of the build process of the local server hosted by the Angular "ng serve"-command.
\\[\baselineskip]
However, one requirement of the development team was not having to think about the script editor web part of the blog application every time the project maintenance was released, which is why it was decided to develop a SharePoint feature \cite{SPFeature} and feature event receiver \cite{SPFeatureEventReceiver}. Furthermore a feature event receiver also solved the question how the data of the old blog application could be migrated automatically.
\\[\baselineskip]
A SharePoint feature, as the name suggests, offers further functionality and can be activated at any time by a user with high enough privileges. A feature event receiver waits until a feature related event shows up to respond to it. Such an event appears when activating, deactivating, installing or deleting a feature. One limitation that has been set up so that elements of the blog feature are not unnecessarily created on the entire system is that the developed feature can only be activated on pages where blogs have already been created. By activating the blog feature, the blog event receiver ensures the creation of all necessary elements and migrates the data from the old blog. The tasks of the blog event receiver are the following:
\begin{itemize}
\item "MigrateBlog": Checks if an old blog is present and if so, the list of blogs and comments is copied to set the respective lists of the new blog application.
\item "EnsurePageLib": Checks the existence of a SharePoint document library \cite{SPDocumentLibrary} on the page of the blog application, if it is not present, one is created. A document library manages files that can be shared among multiple users.
\item "EnsureLandingPage": Checks by means of a configuration file if all necessary web parts of the page have been created, if not they will be created. This ensures the creation of the script editor web part and thereby configures the paths to the compiled project files. In addition, the welcome page in the root folder is set to "Beitraege.aspx".
\item "EnsureCategoryListFields": Ask if a list of categories already exists otherwise a new one will be created. If a list already exists, an additional lookup field is defined and added as this is required for filtering the categories.
\item "EnsureTagsList": Only ensures the creation of a list of tags.
\item "EnsureBlogListFields": Checks if a list has been created for blogs, if not a list will be created. The default blog list will be expanded with fields as they are needed for the new functionality. These include tags, target audiences, content owners, and the time how long a blog post is in the global news section of the SharePoint.
\item "EnsureCategoryItemEventReceiver": Checks the creation of the category item event receiver, if none exists one will be created and configured. This event receiver then ensures that when creating, updating or deleting a category, the category list of alerts is also taken into account.
\item "EnsureTagsItemEventReceiver": Similar to above only this is the event receiver of the tag items.
\item "ReorderFields": This adjusts the default arrangement of the fields in the blog list. The order of the fields not only plays a role when displaying the list but also when creating a new blog. This arrangement was a requirement of the customer.
\item "EnsureAlertManagerGroup": Checks if an alert manager group has been created, if none exist, on will be created. An alert manager has the rights to subscribe to a blog, categories or tags on behalf of others.
\end{itemize}

So that all functionalities of the SharePoint can be used the website must be started in an Internet Explorer \cite{IE}. Since this browser does not support all the features that Angular offers or requires for development, polyfills had to be integrated. Apart from the polyfill, which allows using "NgClass" on SVG elements, all polyfills recommended by angular-cli for the support of Internet Explorer 9-11 were imported. In addition, the "core-js/es7"-polyfill was imported, as it provided the "Array.prototype.includes()" feature of ECMAScript 2016, which increased the readability of some parts of the source code.


\section{Software Architecture}
This section deals with the software architecture of the application. Attention is focused on how the application was built, why these decisions were made and what was gained thereby.
\\[\baselineskip]
The first chapter describes the software components which together form the structure of the project. The following section describes the different interfaces the application needs to communicate with thus the services of the application can query the required data. Finally, how the development of the design of the application has taken place will be discussed. Special attention is given to structuring and avoiding unnecessary boilerplate code.

\subsection{Software Components}
The basis for the Angular project was created by using Angular CLI's \cite{AngularCli} "ng new" command \cite{AngularCli}. When executing the command, interactive prompts ask as to whether a basis for the routing should be generated and which style sheet format should be utilized. The structure created by this command consists of a folder for end-to-end tests, one for the node modules and one for the source code. Furthermore, some configuration files for the project will be added.
\\[\baselineskip]
Using the basic framework generated by Angular CLI as a basis, the development team tried to design a project structure that was both as clear as possible and easy to extend. For this purpose, a module-oriented approach was employed. A module contains several components which together bring functionality. The components were split up into containers and components, partly because of the reactive programming approach but also to have a better overview.
\\[\baselineskip]
A component gets the required data inserted and does not request anything from the store. Moreover, events such as the press of a button are not processed by the component but passed on to the container in which the components live. However, this does not mean that a component can not call other components. In contrast, a container queries the required data by sending actions that first return an observable, which is then passed to a component via the async-pipe \cite{AngularAsyncPipe}. Each container and component has an HTML file for defining the structure of the data, an SCSS \cite{Sass} file for the design of the structure, and a TypeScript file, which takes care of the state of the data to be displayed.
\\[\baselineskip]
So that the actions, reducers, selectors, and effects of the application are also structured, these were housed in the respective modules in which they are called. For actions, reducer, and effects, a separate folder was established inside of the module just like for containers and components. Selectors were added to the folder of the reducers because they were in the beginning in the same file and the developer team did not want to separate them completely.
\\[\baselineskip]
In each module were in addition to a module configuration file, also a routing module and a guard \cite{AngularGuard}. In the routing module, was defined at which routes which component of the module should be called. Furthermore, several guards per route could be designated, which usually made sure that the logged in user has the privileges to access this data and whether the data still exists. If a guard rejects the navigation, the navigation to another page will be terminated, unless the guard returns a "UrlTree", then an attempt is made to navigate to that location.
\\[\baselineskip]
The application was divided into the modules blog, alert, core and shared, as well as the root reducer, which mainly took care of the router state, and the services, which were employed to request the required data. The core module is the entry point of the application and defines the error page as well as the logic for displaying errors and checking the current user. If no error is thrown when starting the application, the error page is not loaded, and a router outlet tries to navigate to the blog page. From then on, after a successful examination of the current user, the features of the blog and the alert module are available to the user. Since a handful of these features consist of components that are used both in the blog and in the alert module, these components became a part of the shared module.


\subsection{Interfaces}
Principally, the developed application needs to communicate with only one other system, which is SharePoint 2013, but with which SharePoint interface has to be spoken changes, depending on where the data is requested from and what the demanded data is. As a result, the technologies that could be utilized to implement a service with change. The different interfaces are, as shown in Figure \ref{Fig7}, the Windows Communication Foundation (WCF) Data Service \cite{SPWcf}, the SharePoint REST API \cite{SPRest}, the People API Service, and the SharePoint File System.
\\[\baselineskip]
\begin{figure}[!htbp]
\centering
\includegraphics[width=0.625\linewidth]{PICs/communication_sharepoint_services.eps}
\caption{The different interfaces required to get all the data from SharePoint}\label{Fig7}
\end{figure}

Six of the nine services were able to request the required data from the SharePoint REST API. The SharePoint REST API expects a request corresponding to the OData (Open Data) protocol. This request is then processed by the "client.svc" web service of the SharePoint and depending on what is specified in the response type of the request, either a response in Atom or JSON format will be returned. In addition to the SharePoint REST API, alternatives such as the Client-Side Object Model (CSOM) API \cite{CSOM} or JavaScript Object Model (JSOM) \cite{JSOM} can be used to query data. However, Microsoft recommends using the SharePoint REST API for client-side customizations, but as long as the REST API offers not all functionality, any alternative is still supported.
\\[\baselineskip]
To keep assembling the queries for the REST API as simple as possible, the fluent JavaScript API, PnPJS \cite{Pnpjs}, developed by Microsoft was utilized. This library allows queries to be generated by chaining methods and then sent to the REST API without having to additionally use an HTTP client. If possible, every service was implemented using this technology.
\\[\baselineskip]
The required functionality of the alert service was not supported by the SharePoint REST API and therefore not by PnPJS, which is why an alternative decision was made to implement a WCF Data Service, as shown in Figure \ref{Fig7}. Since a WCF Data Service also uses the OData protocol, this service could be seen as a custom extension of the SharePoint REST API. For querying data, the Angular HTTP client was injected into the service.
\\[\baselineskip]
The people service had the same problems as the alert service. However, no separate WCF data service had to be developed here because no data was created but only requested. Therefore, employing an HTTP client was enough to communicate with the SharePoint People API service, as illustrated in Figure \ref{Fig7}. However, another limitation occurred when querying data with an Angular HTTP client the request was not accepted unless an authentication token was included in the request. This token could only be obtained by requesting authentication only. However, this did not have to be implemented if the SharePoint HTTP client was employed since it already does the authentication by itself.
\\[\baselineskip]
The rating feature of a blog post should be able to be switchable between the average number of stars and the number of likes given by users. Furthermore, when hovering over the rating, the names of people who have already rated the blog post should be displayed. Finally, hovering over a person should display a window that includes the person's Outlook (Microsoft Corporation, Redmond WA) profile. A request from the customer was that these features are always up to date without reloading the page. Since the SharePoint REST API did not provide any methods for this and the implementation with a WCF Data Service was based on executing scripts in the SharePoint file system, the blog-rating service was implemented using the SharePoint JavaScript Class Library \cite{SpJavaScriptClassLibrary}. As a result, using the SOD (Script On Demand) class \cite{SpSod}, the respective scripts used for the above functionalities could be registered and executed when changing the state of the rating.
\\[\baselineskip]
Before a service returned the response, the data residing on service entities was mapped to an entity convenient to the components. As a result future changes to the services only affect the service entities and the mapping.


\subsection{Styles}
Cascading Style Sheets (CSS) is the standard language used to describe the design of an HTML structure. Over 90\% of web developers use CSS, although some familiar programming constructs such as variables are not supported. Since it often happens that a customer does not like some aspects of the design, such as the color of icons, this often results in an effort that was not entirely planned. This effort increases especially if more CSS code is present than really needed, no structure is defined in the CSS files, and one CSS class affects several elements of the website. In order to keep this effort as low as possible, technologies such as Sass (Syntactically Awesome Style Sheets) \cite{Sass} and Bootstrap \cite{Bootstrap} were included, and the methodology BEM (Block Element Modifier) \cite{BEM} was utilized in defining the structure of the styles.
\\[\baselineskip]
In order to create a responsive website, Bootstrap's Container \cite{BootstrapContainer} and Grid \cite{BootstrapGrid} CSS classes were utilized. Thereby CSS classes that would have to take care of the layout did not have to be developed, leaving the Sassy Cascading Style Sheet (SCSS) files clearer.
\\[\baselineskip]
\begin{figure}[!htbp]
\centering
\includegraphics[width=0.75\linewidth]{PICs/sass_parent-selector_mixins_variables.eps}
\caption{An SCSS file (left) and the resulting CSS file (right) after compilation.}\label{FigSass}
\end{figure}

Sass is a CSS preprocessor, which provides the ability to manage variables, mixins, nesting, and a lot more to define the design of a component. The creators of Sass describe it as "CSS with superpowers" \cite{Sass}. The written code resides in so-called SCSS files and is transformed from the preprocessor compiler to plain CSS as the application builds, as shown in Figure \ref{FigSass} \cite[p.~168]{CSSPreproc}. For the preprocessor to recognize a variable, the name of the variable must start with a "\$" \cite[p.~169]{CSSPreproc} \cite{SassVariables}. Variables were mainly employed for colors because they were required in several places and were changed most frequently during the development. When creating a variable for colors, it was essential to initialize a second variable with the first as its value, as shown in Figure \ref{FigSass} in the SCSS code. Because then the name of the variable only had to be adjusted in the top of the SCSS file where all variables had to be declared. 
\\[\baselineskip]
Furthermore, a Mixin was defined, which was applied to all icon CSS classes so that the size and orientation of them remained the same. How to integrate a mixin into an SCSS file is shown by the "align-icon" mixin in Figure \ref{FigSass} in the SCSS code \cite{SassMixin}. A mixin can specify a group of styles, which can then be utilized throughout the project by including the mixin into the respective CSS class \cite[p.~170]{CSSPreproc} \cite{SassMixin}. In order not to unnecessarily compile a mixin, which is not included in any SCSS class, all mixins were defined in a partial SCSS file. Partial SCSS files can be identified by an underscore at the beginning of the filename \cite{SassPartial}.
\\[\baselineskip]
The nesting feature and the parent selector "\&" allowed to create a hierarchical structure of the SCSS files, which is similar to the HTML structure \cite[p.~169-170]{CSSPreproc} \cite{SassParent} \cite{SassNesting}. This structure was constructed by interlacing multiple SCSS classes together, avoiding some boilerplate code in the process \cite{SassNesting}. Through the parent selector, which references the outer selector, even more boilerplate code was circumvented \cite{SassParent}. As a result, as in Figure \ref{FigSass}, the "\&\_\_ icon" class is compiled into the "blog-tools\_\_icon" class.
\\[\baselineskip]
In order for the structure of the SCSS files to be implemented uniformly, BEM was adopted as a naming convention. The rules that came with it were the following: 
\begin{itemize}
\item Entities that are meaningful by themselves, such as the "div" element that houses the blog tools, are considered as a block. This "div" elements would, therefore, be given the CSS class "blog-tools" \cite{BEMNaming}.
\item Entities that do not make sense on their own are called elements, hence the elements of the blog tool got the class "blog-tools\_\_element" \cite{BEMNaming}.
\item A CSS class that causes a change in the appearance, behavior, or state of an entity is called a modifier. In Figure \ref{FigSass}, the class "blog-tools\_\_element-hidden" describes a modifier that defines the change in visibility of an element of class "blog-tools\_\_element" \cite{BEMNaming}.
\end{itemize}


\section{Reactive Programming}

\subsection{Libraries \& Development Tools}
rxjs,
\\[\baselineskip]
\begin{figure}[!htbp]
\centering
\includegraphics[width=1\linewidth]{PICs/without_and_with_unionize_actions.eps}
\caption{"LoadBlogEntriesAction" implemented without (left) and with the use of the methods "unionize" and "ofType" from the TypeScript library Unionize \cite{Unionize} (right).}\label{Fig4}
\end{figure}
easier use of the redux pattern through Unionize \cite{Unionize} (removing some boilerplate code for writing actions (unionize, ofType), reducers (Action.match) and effects(Actions.is.SpecificAction)).
\\[\baselineskip]
\begin{figure}[!htbp]
\centering
\includegraphics[width=1\linewidth]{PICs/with_and_without_immer_unionize_reducer.eps}
\caption{"BlogReducer" implemented without (left) and with the use of the methods "produce" and "match" from the JavaScript library Immer \cite{Immer} and the TypeScript library Unionize \cite{Unionize} (right).}\label{Fig5}
\end{figure}
Immer \cite{Immer} (removing some boilerplate code for writing reducers (produce)).
more features for reactive programming, Redux DevTools \cite{ReduxDevTools} as browser extension by importing the store devtools of ngrx into the AppModule.

\subsection{Adding a new Feature}
\begin{figure}[!htbp]
\centering
\includegraphics[width=0.75\linewidth]{PICs/ngrx_redux_structure.eps}
\caption{The Redux pattern in conjunction with NgRx}\label{Fig6}
\end{figure}
explain implementation in form of an example like "showing blog entry", Store, Actions, State, Reducer, Selector, Effect, etc., verweis auf \ref{Fig6}

\subsection{Unit Testing}
testing with reactive programming (reducer, effects (jasmine-marbles)); allgemein welche Technologien wurden genutzt: Karma \cite{Karma} (test runner; config for browser, reporters, frameworks, and more); Jasmine \cite{Jasmine} (behavior-driven development framework for testing); Karma-Mocha-Reporter \cite{KarmaMocha} (javascript test framework) and Karma-Jasmine-HTML-Reporter \cite{KarmaJasmineHTML} for reporting; Given-When-Then \cite{GivenWhenThen} format for structuring test cases; 

\clearpage


\chapter{Discussion}

\section{Development}
auch über beantwortung der forschungsfragen schreiben, 
roter faden soll hier gesponnen werden also von dem 3. Kaptiel aufzählen warum man darüber geschrieben hat
Difficulties (no need for PhantomJS anymore because of Chrome as headless browser), 
bessere erweiterbarkeit und höhere software qualität durch "Software Components", 
SharePoint HTTP client could not be injected, 
erweitern um eine base.scss welches variablen definiert damit definition an einem ort ist
use @extend of sass to stay DRY
possible overhead of Redux huge advantages at a cost (also extensions for chrome which makes debugging less painful), 

\section{Results}
presenting finished solution
\begin{figure}[!htbp]
\centering
\includegraphics[width=1\linewidth]{PICs/blog_list_page.eps}
\caption{Blog List page}\label{FigBlogList}
\end{figure}
\begin{figure}[!htbp]
\centering
\includegraphics[width=1\linewidth]{PICs/blog_detail_page.eps}
\caption{Blog Detail page}\label{FigBlogDetail}
\end{figure}
\begin{figure}[!htbp]
\centering
\includegraphics[width=1\linewidth]{PICs/my_blog_alerts_page.eps}
\caption{My Blog Alerts page}\label{FigMyAlerts}
\end{figure}
\begin{figure}[!htbp]
\centering
\includegraphics[width=1\linewidth]{PICs/mandatory_alerts.eps}
\caption{Mandatory Blog Alerts page}\label{FigMandatoryAlerts}
\end{figure}
\clearpage


\chapter{Conclusion \& Future Work}




% Hier beginnen die Verzeichnisse.
\clearpage
\ifthenelse{\equal{\FHTWCitationType}{HARVARD}}{}{\bibliographystyle{gerabbrv}}
\bibliography{Literatur}
\clearpage

% Das Abbildungsverzeichnis
\listoffigures
\clearpage

\phantomsection
\addcontentsline{toc}{chapter}{\listacroname}
\chapter*{\listacroname}
\begin{acronym}[XXXXX]
    \acro{API}[API]{Application Programming Interface}
    \acro{BEM}[BEM]{Block Element Modifier}
    \acro{CLI}[CLI]{Command-Line Interface}
    \acro{CSS}[CSS]{Cascading Style Sheets}
    \acro{ECMA}[ECMA]{European Computer Manufacturers Association}
    \acro{GUI}[GUI]{Graphical User Interface}
    \acro{HTML}[HTML]{Hypertext Markup Language}
    \acro{HTTP}[HTTP]{Hypertext Transfer Protocol}
    \acro{REST}[REST]{Representational State Transfer}
    \acro{SASS}[SASS]{Syntactically Awesome Style Sheets}
    \acro{SCSS}[SCSS]{Sassy Cascading Style Sheets}
    \acro{SVG}[SVG]{Scaleable Vector Graphics}
    \acro{TFS}[TFS]{Team Foundation Server}
    \acro{XML}[XML]{Extensible Markup Language}
\end{acronym}

\end{document}}